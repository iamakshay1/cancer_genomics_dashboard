{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5a6956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "import optuna.visualization as ov\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d27c47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\aksha\\Downloads\\data.csv(1)\\data.csv')\n",
    "labels = pd.read_csv(r'C:\\Users\\aksha\\Downloads\\labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482bc13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA    300\n",
      "KIRC    146\n",
      "LUAD    141\n",
      "PRAD    136\n",
      "COAD     78\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data['labels'] = labels['Class']\n",
    "print(data['labels'].value_counts())\n",
    "tsne_data = data.drop(columns=['Unnamed: 0', 'labels'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1b4bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=3)\n",
    "tsne_res = tsne.fit_transform(tsne_data)\n",
    "tsne_res = pd.DataFrame(tsne_res)\n",
    "tsne_res['labels'] = data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffd0a0",
   "metadata": {},
   "source": [
    "**DASHBOARD 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1b415f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_importance(model, data):\n",
    "    X = data.drop(columns=['Unnamed: 0', 'labels'], axis=1)\n",
    "    y = data['labels']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create an empty DataFrame to store feature importances for each class\n",
    "    all_feature_importances = pd.DataFrame(columns=['Feature', 'Importance', 'Target'])\n",
    "\n",
    "    # Loop through each class and extract top 5 features\n",
    "    for cancer_type in y.unique():\n",
    "        # Fit the model for the current class\n",
    "        model.fit(X_train[y_train == cancer_type], y_train[y_train == cancer_type])\n",
    "\n",
    "        # Get feature importances\n",
    "        feature_importances = model.feature_importances_\n",
    "\n",
    "        # Create a DataFrame with feature names and their importances\n",
    "        feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances, 'Target': cancer_type})\n",
    "\n",
    "        # Sort the DataFrame by importance in descending order\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        # Extract top 5 features for the current class\n",
    "        top_features_for_cancer_type = feature_importance_df.head(10)\n",
    "\n",
    "        # Append the results to the overall DataFrame\n",
    "        all_feature_importances = pd.concat([all_feature_importances, top_features_for_cancer_type])\n",
    "\n",
    "    # Print or inspect the top features for each class\n",
    "    #print(all_feature_importances)\n",
    "    top_features_by_cancer_type = all_feature_importances.groupby('Target')['Feature'].apply(lambda x: ', '.join(x.head(10))).reset_index()\n",
    "    #print(all_feature_importances)\n",
    "    # Print or use the resulting DataFrame\n",
    "    outputs = [\n",
    "        top_features_by_cancer_type.loc[0, 'Target'] + ' : ' + top_features_by_cancer_type.loc[0, 'Feature'],\n",
    "        top_features_by_cancer_type.loc[1, 'Target'] + ' : ' + top_features_by_cancer_type.loc[1, 'Feature'],\n",
    "        top_features_by_cancer_type.loc[2, 'Target'] + ' : ' + top_features_by_cancer_type.loc[2, 'Feature'],\n",
    "        top_features_by_cancer_type.loc[3, 'Target'] + ' : ' + top_features_by_cancer_type.loc[3, 'Feature'],\n",
    "        top_features_by_cancer_type.loc[4, 'Target'] + ' : ' + top_features_by_cancer_type.loc[4, 'Feature']\n",
    "    ]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d42fea-5b56-40c4-8589-15c36863a859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:6551/\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import html, dcc, callback, Input, Output\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap.umap_ import UMAP\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming 'data' is your dataset\n",
    "\n",
    "scaler = Normalizer()\n",
    "reduction_data = data.drop(columns=['Unnamed: 0', 'labels'], axis=1)\n",
    "columns = reduction_data.columns\n",
    "transformed = scaler.fit_transform(reduction_data)\n",
    "reduction_data = pd.DataFrame(transformed, columns=columns)\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP, dbc.icons.BOOTSTRAP])\n",
    "\n",
    "# Dashboard Layout\n",
    "app.layout = dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col(html.H1(\"Dashboard 1\"), width={'size': 6, 'offset': 3})\n",
    "    ]),\n",
    "\n",
    "    dbc.Row([\n",
    "        dbc.Col(\n",
    "            dcc.Graph(id='target_histogram'),\n",
    "            width={'size': 6, 'offset': 0}  # Adjust the width of the histogram column\n",
    "        ),\n",
    "        dbc.Col([\n",
    "            dcc.Dropdown(\n",
    "                options=[\n",
    "                    {'label': 'BRCA', 'value': 'BRCA'},\n",
    "                    {'label': 'COAD', 'value': 'COAD'},\n",
    "                    {'label': 'LUAD', 'value': 'LUAD'},\n",
    "                    {'label': 'PRAD', 'value': 'PRAD'},\n",
    "                    {'label': 'KIRC', 'value': 'KIRC'}\n",
    "                ],\n",
    "                value='BRCA',\n",
    "                id='label_type'\n",
    "            ),\n",
    "            html.Div(id='imp_feat', style={'width': '100%', 'overflowWrap': 'break-word'})\n",
    "        ], width={'size': 6, 'offset': 0})  # Adjust the width of the dropdown + imp_feat column\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Dropdown(\n",
    "            options=[\n",
    "                {'label': 'PCA', 'value': 'PCA'},\n",
    "                {'label': 'LDA', 'value': 'LDA'},\n",
    "                {'label': 't-SNE', 'value': 'TSNE'},\n",
    "                {'label': 'UMAP', 'value': 'UMAP'}\n",
    "            ],\n",
    "            value='PCA',\n",
    "            id='method'\n",
    "        ))\n",
    "    ]),\n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='feature_graph'))\n",
    "    ])\n",
    "])\n",
    "fig = None\n",
    "\n",
    "@app.callback(\n",
    "    Output('feature_graph', 'figure'),\n",
    "    Input('method', 'value')\n",
    ")\n",
    "def update_visualization(method):\n",
    "\n",
    "    if method == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_res = pca.fit_transform(reduction_data)\n",
    "        pca_res = pd.DataFrame(pca_res, columns=['Component 1', 'Component 2'])\n",
    "        pca_res['labels'] = data['labels']\n",
    "        \n",
    "        fig = px.scatter(pca_res, x='Component 1', y='Component 2', color='labels', \n",
    "                         title='PCA Visualization')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    elif method == 'LDA':\n",
    "        lda = LDA(n_components=2)\n",
    "        lda_res = lda.fit_transform(reduction_data, data['labels'])\n",
    "        lda_res = pd.DataFrame(lda_res, columns=['Component 1', 'Component 2'])\n",
    "        lda_res['labels'] = data['labels']\n",
    "        \n",
    "        fig = px.scatter(lda_res, x='Component 1', y='Component 2', color='labels', \n",
    "                         title='LDA Visualization')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    elif method == 'TSNE':\n",
    "        tsne = TSNE(n_components=2)\n",
    "        tsne_res = tsne.fit_transform(reduction_data)\n",
    "        tsne_res = pd.DataFrame(tsne_res, columns=['Component 1', 'Component 2'])\n",
    "        tsne_res['labels'] = data['labels']\n",
    "        \n",
    "        fig = px.scatter(tsne_res, x='Component 1', y='Component 2', color='labels', \n",
    "                         title='t-SNE Visualization')\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    elif method == 'UMAP':\n",
    "        umap = UMAP(n_components=2)\n",
    "        umap_res = umap.fit_transform(reduction_data)\n",
    "        umap_res = pd.DataFrame(umap_res, columns=['Component 1', 'Component 2'])\n",
    "        umap_res['labels'] = data['labels']\n",
    "        \n",
    "        fig = px.scatter(umap_res, x='Component 1', y='Component 2', color='labels', \n",
    "                         title='UMAP Visualization')\n",
    "        return fig\n",
    "    \n",
    "\n",
    "@app.callback(\n",
    "    Output('imp_feat', 'children'),\n",
    "    Input('label_type', 'value')\n",
    ")\n",
    "def update_important_features(label_type):\n",
    "    rf = RandomForestClassifier()  # Initialize the model\n",
    "    top_features_by_cancer_type = get_feature_importance(rf, data)  # Get feature importance for all labels\n",
    "    \n",
    "    # Find the top features for the selected label type\n",
    "    selected_label_features = [entry for entry in top_features_by_cancer_type if label_type in entry]\n",
    "    \n",
    "    # If the selected label type is found in the results\n",
    "    if selected_label_features:\n",
    "        selected_label_features = selected_label_features[0]  # Take the first occurrence\n",
    "        label, features = selected_label_features.split(' : ')\n",
    "        features_list = features.split(', ')  # Split the features by comma\n",
    "        \n",
    "        # Create a numbered list of features\n",
    "        numbered_features = [f'{i+1}. {feature}' for i, feature in enumerate(features_list)]\n",
    "        formatted_features = '\\n'.join(numbered_features)  # Join the list with line breaks\n",
    "        \n",
    "        return html.Div([\n",
    "            html.Strong(f'Genes important for identifying {label}:'),\n",
    "            dcc.Markdown(formatted_features)\n",
    "        ])\n",
    "    else:\n",
    "        return html.Div('Label type not found or no feature importance available for this type.')\n",
    "\n",
    "@app.callback(\n",
    "    Output('target_histogram', 'figure'),\n",
    "    Input('method', 'value')\n",
    ")\n",
    "def update_target_histogram(method):\n",
    "    # Assuming 'target' is your target variable\n",
    "    fig = px.histogram(data, x='labels', title='Histogram of Target Variable')\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=500,  # Adjust the width as needed\n",
    "        height=400  # Adjust the height as needed\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(port=6551,debug=True,jupyter_mode=\"external\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c188ea3-ae40-4aaa-ad61-e3764b455687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################Dashboard 1 ENDS here #################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82f2ef",
   "metadata": {},
   "source": [
    "### Dashboard 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61e5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b98299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit a given model to data using this function\n",
    "def fit_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "# this function will generate classification reports for a given model\n",
    "def get_classification_report(model):\n",
    "    model = fit_model(model)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # save the classification report as a dict, this gives flexibility to read each indivisual metrics\n",
    "    cf = classification_report(y_test, preds, output_dict=True, zero_division=0.0)\n",
    "\n",
    "    # extract f1 scores\n",
    "    f1_scores = [cf[str(label)]['f1-score'] for label in np.unique(y_test)]\n",
    "\n",
    "    # Create a Plotly bar chart\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=np.unique(y_test),\n",
    "        y=f1_scores,\n",
    "        text=f1_scores,\n",
    "        textposition='auto',\n",
    "        marker=dict(color='blue'),\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(title='Class'),\n",
    "        yaxis=dict(title='Precision (F1 Score)'),\n",
    "        title='Precision (F1 Score) for Each Class',\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# we use one versus many method for generating ROC for multiclass classsification\n",
    "def get_roc(model):\n",
    "    # Binarize the labels for each class\n",
    "    y_train_bin = label_binarize(y_train, classes=np.unique(y_train))\n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "\n",
    "    # Train your model (replace with your own model)\n",
    "    model = OneVsRestClassifier(model)  # Replace with your model\n",
    "    model.fit(X_train, y_train_bin)\n",
    "\n",
    "    # Get predicted probabilities on the test set\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "\n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_scores[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Create a Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot the ROC curve for each class\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        fig.add_trace(go.Scatter(x=fpr[i], y=tpr[i], mode='lines',\n",
    "                                 name=f'Class {i} (AUC = {roc_auc[i]:.2f})'))\n",
    "\n",
    "    # Plot the diagonal line\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Diagonal',\n",
    "                             line=dict(dash='dash', color='navy')))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(title='False Positive Rate'),\n",
    "        yaxis=dict(title='True Positive Rate'),\n",
    "        title='Receiver Operating Characteristic (ROC) Curve for Multi-class (One-vs-All)',\n",
    "        legend=dict(x=0, y=1, traceorder='normal'))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc364891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_roc(model_name):\n",
    "    cf = None\n",
    "    roc_fig = None\n",
    "    # we define classification models based on model name accepted from the dropdown\n",
    "    \n",
    "    if model_name == 'Logistic Regreession':\n",
    "        model = LogisticRegression()\n",
    "        model = fit_model(model)\n",
    "        \n",
    "        # get ROC figure\n",
    "        roc_fig = get_roc(model)\n",
    "        \n",
    "        # get classification report figure\n",
    "        cf = get_classification_report(model)\n",
    "        \n",
    "    if model_name == 'Random Forest Classifier':\n",
    "        model = RandomForestClassifier()\n",
    "        model = fit_model(model)\n",
    "        \n",
    "        # get ROC figure\n",
    "        roc_fig = get_roc(model)\n",
    "        \n",
    "        # get classification report figure\n",
    "        cf = get_classification_report(model)\n",
    "        \n",
    "    if model_name == 'XGB Classifier':\n",
    "        model = XGBClassifier(num_class=5)\n",
    "        model = fit_model(model)\n",
    "        \n",
    "        # get ROC figure\n",
    "        roc_fig = get_roc(model)\n",
    "        \n",
    "        # get classification report figure\n",
    "        cf = get_classification_report(model)\n",
    "        \n",
    "    if model_name == 'Bagging Classifier':\n",
    "        model = BaggingClassifier()\n",
    "        model = fit_model(model)\n",
    "        \n",
    "        # get ROC figure\n",
    "        roc_fig = get_roc(model)\n",
    "        \n",
    "        # get classification report figure\n",
    "        cf = get_classification_report(model)\n",
    "        \n",
    "    if model_name == 'ADA Boost Classfier':\n",
    "        model = AdaBoostClassifier()\n",
    "        model = fit_model(model)\n",
    "        \n",
    "        # get ROC figure\n",
    "        roc_fig = get_roc(model)\n",
    "        \n",
    "        # get classification report figure\n",
    "        cf = get_classification_report(model)\n",
    "        \n",
    "    if model_name == 'Decision Tree Classifier':\n",
    "        model = DecisionTreeClassifier()\n",
    "        model = fit_model(model)\n",
    "        \n",
    "        # get ROC figure\n",
    "        roc_fig = get_roc(model)\n",
    "        \n",
    "        # get classification report figure\n",
    "        cf = get_classification_report(model)\n",
    "        \n",
    "    return cf, roc_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aee6c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop('Unnamed: 0', axis = 1, inplace= True)\n",
    "labels.drop('Unnamed: 0', axis = 1, inplace= True)\n",
    "X = data\n",
    "y = labels \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "models = {\n",
    "    'LogisticRegression': {\n",
    "        'C': (1e-5, 1e5),\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'n_estimators': (50, 200),\n",
    "        'learning_rate': (0.01, 1.0),\n",
    "    },\n",
    "    'BaggingClassifier': {\n",
    "        'n_estimators': (10, 100),\n",
    "    },\n",
    "    'RandomForestClassifier': {  # Corrected class name\n",
    "        'n_estimators': (10, 200),\n",
    "        'max_depth': (2, 32),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 10),\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': (2, 32),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 10),\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'learning_rate': (0.01, 1.0),\n",
    "        'n_estimators': (50, 200),\n",
    "        'max_depth': (2, 32),\n",
    "        'min_child_weight': (1e-5, 1e5),\n",
    "        'subsample': (0.1, 1.0),\n",
    "        'colsample_bytree': (0.1, 1.0),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b95751bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 14:16:26,911] A new study created in memory with name: no-name-fc452996-6bbe-4ce9-833e-50113744d39f\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,205] Trial 0 finished with value: 0.9516142184662145 and parameters: {'C': 23821.352369711327}. Best is trial 0 with value: 0.9516142184662145.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,407] Trial 1 finished with value: 0.9579497088835258 and parameters: {'C': 44691.613910531676}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,528] Trial 2 finished with value: 0.9516142184662145 and parameters: {'C': 62797.25596586869}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,621] Trial 3 finished with value: 0.9516142184662145 and parameters: {'C': 65608.62094869223}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,676] Trial 4 finished with value: 0.9516142184662145 and parameters: {'C': 58711.528787275776}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,752] Trial 5 finished with value: 0.9516142184662145 and parameters: {'C': 55799.96300298356}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,844] Trial 6 finished with value: 0.9516142184662145 and parameters: {'C': 21459.628802148487}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:27,990] Trial 7 finished with value: 0.9516142184662145 and parameters: {'C': 94468.19068980106}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,170] Trial 8 finished with value: 0.9578614095285454 and parameters: {'C': 7608.357347240479}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,376] Trial 9 finished with value: 0.9516142184662145 and parameters: {'C': 21059.978996615242}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,523] Trial 10 finished with value: 0.9516142184662145 and parameters: {'C': 86699.40239352128}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,627] Trial 11 finished with value: 0.9517373300927177 and parameters: {'C': 807.0758908250573}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,728] Trial 12 finished with value: 0.9516142184662145 and parameters: {'C': 39904.082247305894}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,837] Trial 13 finished with value: 0.9517373300927177 and parameters: {'C': 3710.854305659459}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:28,960] Trial 14 finished with value: 0.9516142184662145 and parameters: {'C': 39985.19653559856}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,118] Trial 15 finished with value: 0.9516142184662145 and parameters: {'C': 75744.90138278372}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,290] Trial 16 finished with value: 0.9578614095285454 and parameters: {'C': 12821.028984648157}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,458] Trial 17 finished with value: 0.9516142184662145 and parameters: {'C': 40257.35490980523}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,596] Trial 18 finished with value: 0.9516142184662145 and parameters: {'C': 47246.31207653797}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,719] Trial 19 finished with value: 0.9579497088835258 and parameters: {'C': 77948.38733936666}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,822] Trial 20 finished with value: 0.9516142184662145 and parameters: {'C': 78166.16602135057}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:29,976] Trial 21 finished with value: 0.9516142184662145 and parameters: {'C': 31694.046606550924}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,108] Trial 22 finished with value: 0.9516142184662145 and parameters: {'C': 78626.42131101158}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,263] Trial 23 finished with value: 0.9516142184662145 and parameters: {'C': 96222.36999851468}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,392] Trial 24 finished with value: 0.9516142184662145 and parameters: {'C': 69572.03096434353}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,500] Trial 25 finished with value: 0.9516142184662145 and parameters: {'C': 49388.23385822954}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,592] Trial 26 finished with value: 0.9516142184662145 and parameters: {'C': 86963.56768852659}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,677] Trial 27 finished with value: 0.9578614095285454 and parameters: {'C': 11293.630547511022}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,764] Trial 28 finished with value: 0.9578614095285454 and parameters: {'C': 32354.9752177606}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,855] Trial 29 finished with value: 0.9516142184662145 and parameters: {'C': 28847.548296045392}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:30,985] Trial 30 finished with value: 0.9578614095285454 and parameters: {'C': 8733.205070475029}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,133] Trial 31 finished with value: 0.9516142184662145 and parameters: {'C': 14384.537137945628}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,278] Trial 32 finished with value: 0.9578614095285454 and parameters: {'C': 14632.208020993534}. Best is trial 1 with value: 0.9579497088835258.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,402] Trial 33 finished with value: 0.964061156069414 and parameters: {'C': 5867.93129493757}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,518] Trial 34 finished with value: 0.9579497088835258 and parameters: {'C': 24401.52920852995}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,616] Trial 35 finished with value: 0.9516142184662145 and parameters: {'C': 55532.16617448171}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,705] Trial 36 finished with value: 0.9516142184662145 and parameters: {'C': 25044.573660891685}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,798] Trial 37 finished with value: 0.9516142184662145 and parameters: {'C': 66331.71962465387}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:31,909] Trial 38 finished with value: 0.9578614095285454 and parameters: {'C': 19084.393718079016}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,042] Trial 39 finished with value: 0.9516142184662145 and parameters: {'C': 61385.564750264915}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,187] Trial 40 finished with value: 0.9516142184662145 and parameters: {'C': 44469.48481668955}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,327] Trial 41 finished with value: 0.9578614095285454 and parameters: {'C': 8190.300860873143}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,456] Trial 42 finished with value: 0.9517373300927177 and parameters: {'C': 4728.587672315849}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,558] Trial 43 finished with value: 0.9516142184662145 and parameters: {'C': 18319.960595249875}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,652] Trial 44 finished with value: 0.9516142184662145 and parameters: {'C': 26364.389211564183}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,736] Trial 45 finished with value: 0.9517373300927177 and parameters: {'C': 1213.6188686264222}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,835] Trial 46 finished with value: 0.9516142184662145 and parameters: {'C': 35227.08221164632}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:32,938] Trial 47 finished with value: 0.9516142184662145 and parameters: {'C': 21095.85885085487}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:33,051] Trial 48 finished with value: 0.9517373300927177 and parameters: {'C': 5658.280727807083}. Best is trial 33 with value: 0.964061156069414.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "[I 2023-12-17 14:16:33,170] Trial 49 finished with value: 0.9516142184662145 and parameters: {'C': 56714.4470993788}. Best is trial 33 with value: 0.964061156069414.\n",
      "[I 2023-12-17 14:16:33,171] A new study created in memory with name: no-name-422f95b6-9575-4a55-8590-1ef3dabfbb7e\n",
      "[I 2023-12-17 14:16:33,697] Trial 0 finished with value: 0.5780325802886155 and parameters: {'n_estimators': 79, 'learning_rate': 0.09786020956346314}. Best is trial 0 with value: 0.5780325802886155.\n",
      "[I 2023-12-17 14:16:34,176] Trial 1 finished with value: 0.5841088328949988 and parameters: {'n_estimators': 74, 'learning_rate': 0.47813165799452373}. Best is trial 1 with value: 0.5841088328949988.\n",
      "[I 2023-12-17 14:16:35,511] Trial 2 finished with value: 0.8354506532996717 and parameters: {'n_estimators': 198, 'learning_rate': 0.8107396866392222}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:36,769] Trial 3 finished with value: 0.5876905425590184 and parameters: {'n_estimators': 193, 'learning_rate': 0.4740575367217579}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:37,638] Trial 4 finished with value: 0.5841088328949988 and parameters: {'n_estimators': 130, 'learning_rate': 0.4431510605183577}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:38,278] Trial 5 finished with value: 0.5780325802886155 and parameters: {'n_estimators': 96, 'learning_rate': 0.1224101376375297}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:38,645] Trial 6 finished with value: 0.6342785129174543 and parameters: {'n_estimators': 56, 'learning_rate': 0.015852223198602577}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:39,500] Trial 7 finished with value: 0.5624398668214493 and parameters: {'n_estimators': 118, 'learning_rate': 0.37550177154832304}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:40,527] Trial 8 finished with value: 0.5780325802886155 and parameters: {'n_estimators': 141, 'learning_rate': 0.11264422601322328}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:40,923] Trial 9 finished with value: 0.5973451293773986 and parameters: {'n_estimators': 52, 'learning_rate': 0.7173205350982558}. Best is trial 2 with value: 0.8354506532996717.\n",
      "[I 2023-12-17 14:16:42,378] Trial 10 finished with value: 0.8425151477659097 and parameters: {'n_estimators': 193, 'learning_rate': 0.9469521625185446}. Best is trial 10 with value: 0.8425151477659097.\n",
      "[I 2023-12-17 14:16:44,087] Trial 11 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 199, 'learning_rate': 0.9778308891370515}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:45,963] Trial 12 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 169, 'learning_rate': 0.992427585243892}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:47,349] Trial 13 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 165, 'learning_rate': 0.9924373716695801}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:49,278] Trial 14 finished with value: 0.5778094578142425 and parameters: {'n_estimators': 166, 'learning_rate': 0.6475930473052578}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:50,442] Trial 15 finished with value: 0.8141582932406579 and parameters: {'n_estimators': 167, 'learning_rate': 0.8749489718433975}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:51,769] Trial 16 finished with value: 0.5532469169142137 and parameters: {'n_estimators': 177, 'learning_rate': 0.6537431226224757}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:52,953] Trial 17 finished with value: 0.8354506532996717 and parameters: {'n_estimators': 156, 'learning_rate': 0.7965344464089954}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:54,185] Trial 18 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 182, 'learning_rate': 0.9946829610417371}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:55,194] Trial 19 finished with value: 0.5663281482990413 and parameters: {'n_estimators': 147, 'learning_rate': 0.2852414209019388}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:55,964] Trial 20 finished with value: 0.5925705884117983 and parameters: {'n_estimators': 111, 'learning_rate': 0.6044137487755932}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:57,123] Trial 21 finished with value: 0.8293670636130543 and parameters: {'n_estimators': 175, 'learning_rate': 0.9095224526310186}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:58,266] Trial 22 finished with value: 0.8337982738887877 and parameters: {'n_estimators': 160, 'learning_rate': 0.8245866541435689}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:16:59,627] Trial 23 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 185, 'learning_rate': 0.9934327116891417}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:00,887] Trial 24 finished with value: 0.8204518858583131 and parameters: {'n_estimators': 139, 'learning_rate': 0.7445643442342127}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:02,807] Trial 25 finished with value: 0.8373799835528072 and parameters: {'n_estimators': 200, 'learning_rate': 0.8923554202113486}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:04,240] Trial 26 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 150, 'learning_rate': 0.9968110043095846}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:05,528] Trial 27 finished with value: 0.8337982738887877 and parameters: {'n_estimators': 171, 'learning_rate': 0.8686691141615227}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:06,864] Trial 28 finished with value: 0.5890905228409864 and parameters: {'n_estimators': 185, 'learning_rate': 0.5810524309938716}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:07,741] Trial 29 finished with value: 0.8182292389400139 and parameters: {'n_estimators': 131, 'learning_rate': 0.7673944186470427}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:08,885] Trial 30 finished with value: 0.8293670636130543 and parameters: {'n_estimators': 158, 'learning_rate': 0.925099096673839}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:10,152] Trial 31 finished with value: 0.9271222637983194 and parameters: {'n_estimators': 184, 'learning_rate': 0.986096818627309}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:11,656] Trial 32 finished with value: 0.8261735201632635 and parameters: {'n_estimators': 181, 'learning_rate': 0.8484877228758109}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:13,378] Trial 33 finished with value: 0.8425151477659097 and parameters: {'n_estimators': 193, 'learning_rate': 0.9486519324089319}. Best is trial 11 with value: 0.9271222637983194.\n",
      "[I 2023-12-17 14:17:14,952] Trial 34 finished with value: 0.932332116733011 and parameters: {'n_estimators': 200, 'learning_rate': 0.9363554744850102}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:16,249] Trial 35 finished with value: 0.5973451293773986 and parameters: {'n_estimators': 199, 'learning_rate': 0.7074328059368045}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:17,561] Trial 36 finished with value: 0.932332116733011 and parameters: {'n_estimators': 192, 'learning_rate': 0.9304633823337773}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:18,814] Trial 37 finished with value: 0.8354506532996717 and parameters: {'n_estimators': 187, 'learning_rate': 0.8096559876971064}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:20,073] Trial 38 finished with value: 0.5841088328949988 and parameters: {'n_estimators': 195, 'learning_rate': 0.22976804131929152}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:20,811] Trial 39 finished with value: 0.5841088328949988 and parameters: {'n_estimators': 103, 'learning_rate': 0.5373770560856022}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:21,336] Trial 40 finished with value: 0.932332116733011 and parameters: {'n_estimators': 79, 'learning_rate': 0.9294317743934156}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:21,826] Trial 41 finished with value: 0.932332116733011 and parameters: {'n_estimators': 75, 'learning_rate': 0.9312897361087282}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:22,426] Trial 42 finished with value: 0.8293670636130543 and parameters: {'n_estimators': 70, 'learning_rate': 0.9211987909562999}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:23,067] Trial 43 finished with value: 0.8373799835528072 and parameters: {'n_estimators': 80, 'learning_rate': 0.8620810191208224}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:23,722] Trial 44 finished with value: 0.932332116733011 and parameters: {'n_estimators': 91, 'learning_rate': 0.9390500985124685}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:24,331] Trial 45 finished with value: 0.840607049247899 and parameters: {'n_estimators': 86, 'learning_rate': 0.7774301746372559}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:25,018] Trial 46 finished with value: 0.932332116733011 and parameters: {'n_estimators': 64, 'learning_rate': 0.933121416284002}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:25,656] Trial 47 finished with value: 0.8373799835528072 and parameters: {'n_estimators': 91, 'learning_rate': 0.8414677704699902}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:26,317] Trial 48 finished with value: 0.5841088328949988 and parameters: {'n_estimators': 63, 'learning_rate': 0.451691066992713}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:27,007] Trial 49 finished with value: 0.5833498574519369 and parameters: {'n_estimators': 78, 'learning_rate': 0.7040432814210776}. Best is trial 34 with value: 0.932332116733011.\n",
      "[I 2023-12-17 14:17:27,008] A new study created in memory with name: no-name-58436482-03f3-4445-aea3-01979bd5a614\n",
      "[I 2023-12-17 14:17:27,396] Trial 0 finished with value: 0.9567175852780286 and parameters: {'n_estimators': 36}. Best is trial 0 with value: 0.9567175852780286.\n",
      "[I 2023-12-17 14:17:27,772] Trial 1 finished with value: 0.964061156069414 and parameters: {'n_estimators': 41}. Best is trial 1 with value: 0.964061156069414.\n",
      "[I 2023-12-17 14:17:28,040] Trial 2 finished with value: 0.964061156069414 and parameters: {'n_estimators': 30}. Best is trial 1 with value: 0.964061156069414.\n",
      "[I 2023-12-17 14:17:28,155] Trial 3 finished with value: 0.964061156069414 and parameters: {'n_estimators': 13}. Best is trial 1 with value: 0.964061156069414.\n",
      "[I 2023-12-17 14:17:28,518] Trial 4 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 51}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:29,078] Trial 5 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 85}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:29,659] Trial 6 finished with value: 0.969132316958404 and parameters: {'n_estimators': 71}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:30,376] Trial 7 finished with value: 0.969132316958404 and parameters: {'n_estimators': 93}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:30,735] Trial 8 finished with value: 0.9634118258097665 and parameters: {'n_estimators': 51}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:31,423] Trial 9 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 94}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:32,004] Trial 10 finished with value: 0.964061156069414 and parameters: {'n_estimators': 68}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:32,573] Trial 11 finished with value: 0.964061156069414 and parameters: {'n_estimators': 74}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:33,248] Trial 12 finished with value: 0.969132316958404 and parameters: {'n_estimators': 59}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:33,730] Trial 13 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 86}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:34,189] Trial 14 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 53}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:34,736] Trial 15 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 81}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:34,909] Trial 16 finished with value: 0.9634118258097665 and parameters: {'n_estimators': 18}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:35,613] Trial 17 finished with value: 0.969132316958404 and parameters: {'n_estimators': 100}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:36,091] Trial 18 finished with value: 0.969132316958404 and parameters: {'n_estimators': 62}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:36,440] Trial 19 finished with value: 0.969132316958404 and parameters: {'n_estimators': 45}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:36,998] Trial 20 finished with value: 0.964061156069414 and parameters: {'n_estimators': 80}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:37,652] Trial 21 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 94}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:38,287] Trial 22 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 88}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:38,486] Trial 23 finished with value: 0.9577642332561889 and parameters: {'n_estimators': 28}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:39,164] Trial 24 finished with value: 0.969132316958404 and parameters: {'n_estimators': 98}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:39,610] Trial 25 finished with value: 0.9579497088835258 and parameters: {'n_estimators': 64}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:40,162] Trial 26 finished with value: 0.9634118258097665 and parameters: {'n_estimators': 79}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:40,771] Trial 27 finished with value: 0.9634118258097665 and parameters: {'n_estimators': 88}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:41,134] Trial 28 finished with value: 0.9628927585609524 and parameters: {'n_estimators': 51}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:41,418] Trial 29 finished with value: 0.969132316958404 and parameters: {'n_estimators': 37}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:42,015] Trial 30 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 73}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:42,622] Trial 31 finished with value: 0.969132316958404 and parameters: {'n_estimators': 87}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:43,279] Trial 32 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 84}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:44,205] Trial 33 finished with value: 0.9634118258097665 and parameters: {'n_estimators': 95}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:44,552] Trial 34 finished with value: 0.969132316958404 and parameters: {'n_estimators': 45}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:45,225] Trial 35 finished with value: 0.9634365102500331 and parameters: {'n_estimators': 91}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:45,677] Trial 36 finished with value: 0.969132316958404 and parameters: {'n_estimators': 68}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:46,245] Trial 37 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 77}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:46,640] Trial 38 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 59}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:46,841] Trial 39 finished with value: 0.969132316958404 and parameters: {'n_estimators': 28}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:47,197] Trial 40 finished with value: 0.969132316958404 and parameters: {'n_estimators': 45}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:47,559] Trial 41 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 52}. Best is trial 4 with value: 0.9696770161959399.\n",
      "[I 2023-12-17 14:17:47,969] Trial 42 finished with value: 0.9757981075227822 and parameters: {'n_estimators': 56}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:48,534] Trial 43 finished with value: 0.964061156069414 and parameters: {'n_estimators': 84}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:48,686] Trial 44 finished with value: 0.9571163620534548 and parameters: {'n_estimators': 22}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:49,182] Trial 45 finished with value: 0.963461239453678 and parameters: {'n_estimators': 68}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:49,444] Trial 46 finished with value: 0.9628927585609524 and parameters: {'n_estimators': 39}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:50,103] Trial 47 finished with value: 0.969132316958404 and parameters: {'n_estimators': 97}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:50,493] Trial 48 finished with value: 0.9696770161959399 and parameters: {'n_estimators': 57}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:51,029] Trial 49 finished with value: 0.9634118258097665 and parameters: {'n_estimators': 76}. Best is trial 42 with value: 0.9757981075227822.\n",
      "[I 2023-12-17 14:17:51,030] A new study created in memory with name: no-name-a24cafac-ecc6-4344-9a38-e2dc87c80059\n",
      "[I 2023-12-17 14:17:51,441] Trial 0 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 75, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9517373300927177.\n",
      "[I 2023-12-17 14:17:52,441] Trial 1 finished with value: 0.963461239453678 and parameters: {'n_estimators': 117, 'max_depth': 21, 'min_samples_split': 13, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:52,916] Trial 2 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 71, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:54,154] Trial 3 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 197, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:54,771] Trial 4 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 94, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:55,549] Trial 5 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 152, 'max_depth': 29, 'min_samples_split': 16, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:56,371] Trial 6 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 172, 'max_depth': 6, 'min_samples_split': 17, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:56,981] Trial 7 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 108, 'max_depth': 26, 'min_samples_split': 9, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:57,476] Trial 8 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:58,252] Trial 9 finished with value: 0.963461239453678 and parameters: {'n_estimators': 125, 'max_depth': 21, 'min_samples_split': 14, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:58,351] Trial 10 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 13, 'max_depth': 12, 'min_samples_split': 20, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:59,074] Trial 11 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 138, 'max_depth': 31, 'min_samples_split': 12, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:17:59,786] Trial 12 finished with value: 0.9629557587969685 and parameters: {'n_estimators': 130, 'max_depth': 18, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:00,150] Trial 13 finished with value: 0.963461239453678 and parameters: {'n_estimators': 56, 'max_depth': 24, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:00,782] Trial 14 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 123, 'max_depth': 14, 'min_samples_split': 19, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:01,577] Trial 15 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 159, 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:01,696] Trial 16 finished with value: 0.9572500593294544 and parameters: {'n_estimators': 36, 'max_depth': 27, 'min_samples_split': 13, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:02,515] Trial 17 finished with value: 0.963461239453678 and parameters: {'n_estimators': 184, 'max_depth': 11, 'min_samples_split': 18, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:03,279] Trial 18 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 117, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:04,043] Trial 19 finished with value: 0.8269402591451054 and parameters: {'n_estimators': 146, 'max_depth': 2, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:04,383] Trial 20 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 69, 'max_depth': 32, 'min_samples_split': 15, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:04,753] Trial 21 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 48, 'max_depth': 23, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:05,194] Trial 22 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 88, 'max_depth': 26, 'min_samples_split': 14, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:05,468] Trial 23 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 47, 'max_depth': 20, 'min_samples_split': 17, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:06,091] Trial 24 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 113, 'max_depth': 24, 'min_samples_split': 13, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:06,173] Trial 25 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 15, 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:06,712] Trial 26 finished with value: 0.963461239453678 and parameters: {'n_estimators': 82, 'max_depth': 28, 'min_samples_split': 16, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:06,991] Trial 27 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 58, 'max_depth': 22, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:07,160] Trial 28 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 32, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:07,865] Trial 29 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 134, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:08,669] Trial 30 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 161, 'max_depth': 30, 'min_samples_split': 18, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:09,610] Trial 31 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 200, 'max_depth': 11, 'min_samples_split': 19, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:10,469] Trial 32 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 186, 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:10,972] Trial 33 finished with value: 0.9510254288740393 and parameters: {'n_estimators': 74, 'max_depth': 16, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:11,836] Trial 34 finished with value: 0.9510254288740393 and parameters: {'n_estimators': 175, 'max_depth': 25, 'min_samples_split': 18, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:12,411] Trial 35 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 98, 'max_depth': 22, 'min_samples_split': 16, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:12,736] Trial 36 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 62, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:13,608] Trial 37 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 181, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:14,333] Trial 38 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 144, 'max_depth': 19, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:14,899] Trial 39 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 105, 'max_depth': 4, 'min_samples_split': 18, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:15,549] Trial 40 finished with value: 0.963461239453678 and parameters: {'n_estimators': 128, 'max_depth': 14, 'min_samples_split': 16, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:16,064] Trial 41 finished with value: 0.9578614095285454 and parameters: {'n_estimators': 77, 'max_depth': 29, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:16,512] Trial 42 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 88, 'max_depth': 28, 'min_samples_split': 16, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:17,050] Trial 43 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 90, 'max_depth': 24, 'min_samples_split': 14, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:17,521] Trial 44 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 80, 'max_depth': 21, 'min_samples_split': 12, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:18,291] Trial 45 finished with value: 0.9517373300927177 and parameters: {'n_estimators': 158, 'max_depth': 26, 'min_samples_split': 17, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:18,960] Trial 46 finished with value: 0.963461239453678 and parameters: {'n_estimators': 117, 'max_depth': 28, 'min_samples_split': 19, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:19,268] Trial 47 finished with value: 0.963461239453678 and parameters: {'n_estimators': 60, 'max_depth': 24, 'min_samples_split': 13, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:19,934] Trial 48 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 100, 'max_depth': 18, 'min_samples_split': 16, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:20,090] Trial 49 finished with value: 0.9572366089982629 and parameters: {'n_estimators': 26, 'max_depth': 16, 'min_samples_split': 14, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.963461239453678.\n",
      "[I 2023-12-17 14:18:20,092] A new study created in memory with name: no-name-e71c0fa3-abf9-43e2-ba34-27d5c003d5d9\n",
      "[I 2023-12-17 14:18:20,105] Trial 0 finished with value: 0.9633577438536851 and parameters: {'max_depth': 28, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9633577438536851.\n",
      "[I 2023-12-17 14:18:20,122] Trial 1 finished with value: 0.9448031489619393 and parameters: {'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.9633577438536851.\n",
      "[I 2023-12-17 14:18:20,154] Trial 2 finished with value: 0.9384695719305312 and parameters: {'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.9633577438536851.\n",
      "[I 2023-12-17 14:18:20,181] Trial 3 finished with value: 0.9695329171366088 and parameters: {'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,198] Trial 4 finished with value: 0.8173227181492504 and parameters: {'max_depth': 3, 'min_samples_split': 11, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,220] Trial 5 finished with value: 0.9448031489619393 and parameters: {'max_depth': 14, 'min_samples_split': 20, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,243] Trial 6 finished with value: 0.9695329171366088 and parameters: {'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,275] Trial 7 finished with value: 0.9254906455482355 and parameters: {'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,296] Trial 8 finished with value: 0.8173227181492504 and parameters: {'max_depth': 3, 'min_samples_split': 12, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,322] Trial 9 finished with value: 0.9254906455482355 and parameters: {'max_depth': 32, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,358] Trial 10 finished with value: 0.9633577438536851 and parameters: {'max_depth': 8, 'min_samples_split': 16, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,411] Trial 11 finished with value: 0.9695329171366088 and parameters: {'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,452] Trial 12 finished with value: 0.9695329171366088 and parameters: {'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,497] Trial 13 finished with value: 0.9632374533802971 and parameters: {'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,544] Trial 14 finished with value: 0.9633577438536851 and parameters: {'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,596] Trial 15 finished with value: 0.9510254288740393 and parameters: {'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,658] Trial 16 finished with value: 0.9632374533802971 and parameters: {'max_depth': 28, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,702] Trial 17 finished with value: 0.9633577438536851 and parameters: {'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,740] Trial 18 finished with value: 0.9633577438536851 and parameters: {'max_depth': 20, 'min_samples_split': 13, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,782] Trial 19 finished with value: 0.9695329171366088 and parameters: {'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,825] Trial 20 finished with value: 0.9384695719305312 and parameters: {'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,874] Trial 21 finished with value: 0.9695329171366088 and parameters: {'max_depth': 24, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,920] Trial 22 finished with value: 0.9633577438536851 and parameters: {'max_depth': 26, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:20,984] Trial 23 finished with value: 0.9633577438536851 and parameters: {'max_depth': 21, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,032] Trial 24 finished with value: 0.9633577438536851 and parameters: {'max_depth': 25, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,086] Trial 25 finished with value: 0.9695329171366088 and parameters: {'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,140] Trial 26 finished with value: 0.9633577438536851 and parameters: {'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,184] Trial 27 finished with value: 0.9633577438536851 and parameters: {'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,224] Trial 28 finished with value: 0.9384695719305312 and parameters: {'max_depth': 28, 'min_samples_split': 4, 'min_samples_leaf': 10}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,259] Trial 29 finished with value: 0.9633577438536851 and parameters: {'max_depth': 18, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,294] Trial 30 finished with value: 0.9633577438536851 and parameters: {'max_depth': 27, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,326] Trial 31 finished with value: 0.9695329171366088 and parameters: {'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,356] Trial 32 finished with value: 0.9633577438536851 and parameters: {'max_depth': 23, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,396] Trial 33 finished with value: 0.9448031489619393 and parameters: {'max_depth': 22, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,436] Trial 34 finished with value: 0.9633577438536851 and parameters: {'max_depth': 29, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,490] Trial 35 finished with value: 0.9633577438536851 and parameters: {'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,550] Trial 36 finished with value: 0.9633577438536851 and parameters: {'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,589] Trial 37 finished with value: 0.9448031489619393 and parameters: {'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,634] Trial 38 finished with value: 0.9384695719305312 and parameters: {'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,688] Trial 39 finished with value: 0.9632374533802971 and parameters: {'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,748] Trial 40 finished with value: 0.9633577438536851 and parameters: {'max_depth': 25, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,809] Trial 41 finished with value: 0.9633577438536851 and parameters: {'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,883] Trial 42 finished with value: 0.9695329171366088 and parameters: {'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:21,947] Trial 43 finished with value: 0.9510254288740393 and parameters: {'max_depth': 5, 'min_samples_split': 19, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,008] Trial 44 finished with value: 0.9695329171366088 and parameters: {'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,079] Trial 45 finished with value: 0.9695329171366088 and parameters: {'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,143] Trial 46 finished with value: 0.8173227181492504 and parameters: {'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,192] Trial 47 finished with value: 0.9695329171366088 and parameters: {'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,246] Trial 48 finished with value: 0.9510254288740393 and parameters: {'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,290] Trial 49 finished with value: 0.9633577438536851 and parameters: {'max_depth': 22, 'min_samples_split': 12, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9695329171366088.\n",
      "[I 2023-12-17 14:18:22,317] A new study created in memory with name: no-name-afca373f-0a16-4baa-807b-ca1dd2b28b05\n",
      "[I 2023-12-17 14:18:22,678] Trial 0 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.32232257152984367, 'n_estimators': 118, 'max_depth': 18, 'min_child_weight': 5101.183377311253, 'subsample': 0.37749415982756984, 'colsample_bytree': 0.9397868967583606}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:24,174] Trial 1 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.3017199020321603, 'n_estimators': 136, 'max_depth': 30, 'min_child_weight': 62187.15195406219, 'subsample': 0.6546245323186292, 'colsample_bytree': 0.16673996488799958}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:25,664] Trial 2 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.3123823970034022, 'n_estimators': 109, 'max_depth': 28, 'min_child_weight': 1183.473190057384, 'subsample': 0.9071594748503794, 'colsample_bytree': 0.5600599328928233}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:26,184] Trial 3 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.5361106387927401, 'n_estimators': 124, 'max_depth': 2, 'min_child_weight': 75437.64292839184, 'subsample': 0.2368361211058445, 'colsample_bytree': 0.840416609742453}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:26,761] Trial 4 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.7828484662369205, 'n_estimators': 190, 'max_depth': 11, 'min_child_weight': 84365.89409329997, 'subsample': 0.1191431186346705, 'colsample_bytree': 0.8191512136093143}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:27,232] Trial 5 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.49337149118901136, 'n_estimators': 98, 'max_depth': 24, 'min_child_weight': 55455.48685906369, 'subsample': 0.7879047988097961, 'colsample_bytree': 0.7619583187974528}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:27,514] Trial 6 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.13592478903728974, 'n_estimators': 83, 'max_depth': 10, 'min_child_weight': 56809.668349893065, 'subsample': 0.9455587905782957, 'colsample_bytree': 0.4635475229456918}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:27,735] Trial 7 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.5967936990393743, 'n_estimators': 61, 'max_depth': 3, 'min_child_weight': 96669.53101230942, 'subsample': 0.8515957443965757, 'colsample_bytree': 0.1837752244010833}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:28,037] Trial 8 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.45706168490968846, 'n_estimators': 89, 'max_depth': 30, 'min_child_weight': 24720.576899438925, 'subsample': 0.7562892327533132, 'colsample_bytree': 0.2485348037342908}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:28,584] Trial 9 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.8937241132070662, 'n_estimators': 146, 'max_depth': 32, 'min_child_weight': 34591.132158607616, 'subsample': 0.46641493328177486, 'colsample_bytree': 0.23442933905447108}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:29,288] Trial 10 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.07164445320424856, 'n_estimators': 166, 'max_depth': 19, 'min_child_weight': 255.49106166596903, 'subsample': 0.4214548418510712, 'colsample_bytree': 0.9947671444763846}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:29,757] Trial 11 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.22059083848418726, 'n_estimators': 139, 'max_depth': 20, 'min_child_weight': 33849.781690662436, 'subsample': 0.6449944285454835, 'colsample_bytree': 0.4407701064564528}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:30,371] Trial 12 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.33040872131832594, 'n_estimators': 151, 'max_depth': 13, 'min_child_weight': 69570.08449259873, 'subsample': 0.3438596575333754, 'colsample_bytree': 0.6428017599172322}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:30,830] Trial 13 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.3784258264578493, 'n_estimators': 123, 'max_depth': 25, 'min_child_weight': 17814.86126508732, 'subsample': 0.5922693462249563, 'colsample_bytree': 0.11056848530274843}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:31,605] Trial 14 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.664852691517836, 'n_estimators': 186, 'max_depth': 24, 'min_child_weight': 45693.82115435243, 'subsample': 0.5267983740061791, 'colsample_bytree': 0.3616477028661481}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:32,619] Trial 15 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.19458405400876325, 'n_estimators': 170, 'max_depth': 15, 'min_child_weight': 66957.1071738014, 'subsample': 0.6742889101100175, 'colsample_bytree': 0.9927052305481564}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:33,142] Trial 16 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.2651665242147756, 'n_estimators': 66, 'max_depth': 7, 'min_child_weight': 47028.86314583737, 'subsample': 0.3288778943514708, 'colsample_bytree': 0.674409137515074}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:33,839] Trial 17 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.4049573742839846, 'n_estimators': 116, 'max_depth': 18, 'min_child_weight': 14710.381440459561, 'subsample': 0.21698315763199777, 'colsample_bytree': 0.3477059512745422}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:34,747] Trial 18 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.011132451168815227, 'n_estimators': 136, 'max_depth': 22, 'min_child_weight': 84653.67901084563, 'subsample': 0.5244616546038385, 'colsample_bytree': 0.8994145709787859}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:35,675] Trial 19 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.6994142366360583, 'n_estimators': 163, 'max_depth': 27, 'min_child_weight': 35873.372674392085, 'subsample': 0.6987057243675493, 'colsample_bytree': 0.5648900986691576}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:36,142] Trial 20 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.9990333759108754, 'n_estimators': 105, 'max_depth': 16, 'min_child_weight': 11715.766414316087, 'subsample': 0.41386946604158886, 'colsample_bytree': 0.6915981816240065}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:36,578] Trial 21 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.3026291573655217, 'n_estimators': 111, 'max_depth': 29, 'min_child_weight': 892.1959877001927, 'subsample': 0.8591481909311868, 'colsample_bytree': 0.5531559650749753}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:37,024] Trial 22 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.1490880981300439, 'n_estimators': 130, 'max_depth': 28, 'min_child_weight': 8644.481577701234, 'subsample': 0.972922131623017, 'colsample_bytree': 0.45363945296786634}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:37,462] Trial 23 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.38338466997808684, 'n_estimators': 99, 'max_depth': 31, 'min_child_weight': 7139.44948514656, 'subsample': 0.8749372463652784, 'colsample_bytree': 0.32705047933626247}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:37,780] Trial 24 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.2771374759358323, 'n_estimators': 77, 'max_depth': 27, 'min_child_weight': 25245.64087006997, 'subsample': 0.6055276882826282, 'colsample_bytree': 0.10380570110610668}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:38,317] Trial 25 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.451946985690445, 'n_estimators': 152, 'max_depth': 21, 'min_child_weight': 20632.706100763673, 'subsample': 0.7328466977830214, 'colsample_bytree': 0.9170425949761348}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:38,711] Trial 26 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.34120061877972474, 'n_estimators': 118, 'max_depth': 32, 'min_child_weight': 55927.780145622586, 'subsample': 0.2567718327831735, 'colsample_bytree': 0.7440275110612303}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:38,992] Trial 27 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.5566405901417294, 'n_estimators': 50, 'max_depth': 26, 'min_child_weight': 64138.04687872283, 'subsample': 0.8070613741682827, 'colsample_bytree': 0.5749516159224977}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:39,475] Trial 28 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.2178185713471265, 'n_estimators': 104, 'max_depth': 23, 'min_child_weight': 42041.46004072581, 'subsample': 0.4853231013167293, 'colsample_bytree': 0.4926092106720882}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:40,043] Trial 29 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.10894950540548884, 'n_estimators': 127, 'max_depth': 29, 'min_child_weight': 77048.7306866682, 'subsample': 0.9090301041387663, 'colsample_bytree': 0.8422893983150952}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:40,533] Trial 30 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.5338057760502098, 'n_estimators': 91, 'max_depth': 2, 'min_child_weight': 27989.86505358435, 'subsample': 0.34698917830386067, 'colsample_bytree': 0.6261939696212357}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:41,480] Trial 31 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.4382506795311596, 'n_estimators': 113, 'max_depth': 5, 'min_child_weight': 81169.01065428887, 'subsample': 0.20362764996629035, 'colsample_bytree': 0.8371444490374527}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:42,162] Trial 32 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.673477185597287, 'n_estimators': 136, 'max_depth': 11, 'min_child_weight': 88859.17163513709, 'subsample': 0.10173869317996923, 'colsample_bytree': 0.7824217291386748}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:42,710] Trial 33 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.5858554701007462, 'n_estimators': 125, 'max_depth': 7, 'min_child_weight': 72284.27724254761, 'subsample': 0.1817146860577191, 'colsample_bytree': 0.9190620134808491}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:43,349] Trial 34 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.48786521411527695, 'n_estimators': 145, 'max_depth': 14, 'min_child_weight': 60763.14982918082, 'subsample': 0.2849324366802491, 'colsample_bytree': 0.7361866384232965}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:43,747] Trial 35 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.7749818628084362, 'n_estimators': 95, 'max_depth': 11, 'min_child_weight': 52901.74172334511, 'subsample': 0.39986381763095397, 'colsample_bytree': 0.8706127810811265}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:44,244] Trial 36 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.5056826274964059, 'n_estimators': 80, 'max_depth': 9, 'min_child_weight': 94564.83690076857, 'subsample': 0.7861591995816695, 'colsample_bytree': 0.9542082173262446}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:45,811] Trial 37 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.3491201043830733, 'n_estimators': 107, 'max_depth': 25, 'min_child_weight': 74742.62486259242, 'subsample': 0.9957586668816012, 'colsample_bytree': 0.7835401349723338}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:46,972] Trial 38 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.26353651354755636, 'n_estimators': 158, 'max_depth': 17, 'min_child_weight': 4778.666810070897, 'subsample': 0.14967337497047556, 'colsample_bytree': 0.40143435343696005}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:47,471] Trial 39 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.6308418585979987, 'n_estimators': 120, 'max_depth': 30, 'min_child_weight': 59122.78353489072, 'subsample': 0.27690529738668723, 'colsample_bytree': 0.26206900036571057}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:48,194] Trial 40 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.4170085323551275, 'n_estimators': 180, 'max_depth': 4, 'min_child_weight': 99274.97443418285, 'subsample': 0.46104525132666707, 'colsample_bytree': 0.49966296611664746}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:48,946] Trial 41 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.7514830510726694, 'n_estimators': 196, 'max_depth': 8, 'min_child_weight': 87990.27219516311, 'subsample': 0.1239725602007428, 'colsample_bytree': 0.7987398706722665}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:49,482] Trial 42 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.7919669530969867, 'n_estimators': 132, 'max_depth': 13, 'min_child_weight': 79574.83669292698, 'subsample': 0.23764331558094315, 'colsample_bytree': 0.18711972459580942}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:50,212] Trial 43 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.9097641573706459, 'n_estimators': 176, 'max_depth': 6, 'min_child_weight': 68260.32846440123, 'subsample': 0.3200569417908113, 'colsample_bytree': 0.8467269220857361}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:51,300] Trial 44 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.1746165577273183, 'n_estimators': 194, 'max_depth': 19, 'min_child_weight': 91213.28240111074, 'subsample': 0.15283123404498128, 'colsample_bytree': 0.9584070042105579}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:52,388] Trial 45 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.8793367296322029, 'n_estimators': 141, 'max_depth': 11, 'min_child_weight': 63011.76197791519, 'subsample': 0.38046394826991903, 'colsample_bytree': 0.6364380083466197}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:52,772] Trial 46 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.25413300273197753, 'n_estimators': 88, 'max_depth': 2, 'min_child_weight': 1358.3743963880333, 'subsample': 0.6000462308285958, 'colsample_bytree': 0.695554599166837}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:53,631] Trial 47 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.30859783548939956, 'n_estimators': 147, 'max_depth': 21, 'min_child_weight': 81976.86140218028, 'subsample': 0.9221573551402013, 'colsample_bytree': 0.991237217886777}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:53,961] Trial 48 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.062280801110583234, 'n_estimators': 73, 'max_depth': 4, 'min_child_weight': 51580.83500235423, 'subsample': 0.5614632270115801, 'colsample_bytree': 0.28772303237730457}. Best is trial 0 with value: 0.19081652911313424.\n",
      "[I 2023-12-17 14:18:54,451] Trial 49 finished with value: 0.19081652911313424 and parameters: {'learning_rate': 0.35501249365488996, 'n_estimators': 99, 'max_depth': 16, 'min_child_weight': 71577.24865685684, 'subsample': 0.18901301193808123, 'colsample_bytree': 0.882942335027062}. Best is trial 0 with value: 0.19081652911313424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:9989/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: RandomForestClassifier\n",
      "Selected Model: RandomForestClassifier\n",
      "Selected Model: RandomForestClassifier\n",
      "Selected Model: XGBClassifier\n",
      "Selected Model: RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "from dash import Dash\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "scaler = Normalizer()\n",
    "reduction_data = data.drop(columns=['labels'], axis=1)\n",
    "columns = reduction_data.columns\n",
    "transformed = scaler.fit_transform(reduction_data)\n",
    "reduction_data = pd.DataFrame(transformed, columns=columns)\n",
    "\n",
    "pca_res = pca.fit_transform(reduction_data)\n",
    "pca_res = pd.DataFrame(pca_res)\n",
    "pca_res['labels'] = data['labels']\n",
    "\n",
    "X = pca_res.drop(columns=['labels'], axis=1)\n",
    "y = data['labels']\n",
    "y = enc.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            options=['PCA', 'UMAP', 'TSNE', 'LDA'],\n",
    "            value='PCA',\n",
    "            id='method'\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "            options=['Logistic Regression', 'Random Forest Classifier', 'XGB Classifier', 'Bagging Classifier', 'ADA Boost Classfier', 'Decision Tree Classifier'],\n",
    "            value='Random Forest Classifier',\n",
    "            id='classification_model'\n",
    "        ),\n",
    "        html.Div([\n",
    "            dcc.Graph(id='classification_report'),\n",
    "            dcc.Graph(id='roc_curve')\n",
    "        ], style={'display': 'flex'}),\n",
    "    ]),\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='model-dropdown',\n",
    "            options=[{'label': model_name, 'value': model_name} for model_name in models.keys()],\n",
    "            value='RandomForestClassifier',  # Initial model\n",
    "            style={'width': '70%'}\n",
    "        ),\n",
    "        dcc.Graph(id='parallel-coordinate-plot'),\n",
    "    ])\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('classification_report', 'figure'), Output('roc_curve', 'figure')], \n",
    "    [Input('method', 'value'), Input('classification_model', 'value')]\n",
    ")\n",
    "\n",
    "def get_model_details(method, model_name):\n",
    "    if method == 'PCA':\n",
    "        pca = PCA()\n",
    "        pca.fit(reduction_data)\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "        # Determine the number of components to retain\n",
    "        n_components = np.argmax(cumulative_variance >= 0.995) + 1\n",
    "        #print('pca : ', n_components)\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_res = pca.fit_transform(reduction_data)\n",
    "        pca_res = pd.DataFrame(pca_res)\n",
    "        pca_res['labels'] = data['labels']\n",
    "\n",
    "        X = pca_res.drop(columns=['labels'], axis=1)\n",
    "        y = pca_res['labels']\n",
    "        y = enc.fit_transform(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if method == 'UMAP':\n",
    "        pca = PCA()\n",
    "        pca.fit(reduction_data)\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        # Determine the number of components to retain\n",
    "        n_components = np.argmax(cumulative_variance >= 0.995) + 1\n",
    "        #print('umap : ', n_components)\n",
    "        umap = UMAP(n_components=n_components)\n",
    "        umap_res = umap.fit_transform(reduction_data)\n",
    "        umap_res = pd.DataFrame(umap_res)\n",
    "        umap_res['labels'] = data['labels']\n",
    "\n",
    "        X = umap_res.drop(columns=['labels'], axis=1)\n",
    "        y = umap_res['labels']\n",
    "        y = enc.fit_transform(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if method == 'TSNE':\n",
    "        pca = PCA()\n",
    "        pca.fit(reduction_data)\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        # Determine the number of components to retain\n",
    "        n_components = np.argmax(cumulative_variance >= 0.995) + 1\n",
    "        tsne = TSNE(n_components=2)\n",
    "        tsne_res = tsne.fit_transform(reduction_data)\n",
    "        tsne_res = pd.DataFrame(tsne_res)\n",
    "        tsne_res['labels'] = data['labels']\n",
    "\n",
    "        X = tsne_res.drop(columns=['labels'], axis=1)\n",
    "        y = tsne_res['labels']\n",
    "        y = enc.fit_transform(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if method == 'LDA':\n",
    "        lda = LDA(n_components=2)\n",
    "        lda_res = lda.fit_transform(reduction_data, data['labels'])\n",
    "        lda_res = pd.DataFrame(lda_res)\n",
    "        lda_res['labels'] = data['labels']\n",
    "\n",
    "        X = lda_res.drop(columns=['labels'], axis=1)\n",
    "        y = lda_res['labels']\n",
    "        y = enc.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    cf, roc = get_cf_roc(model_name)\n",
    "    return cf, roc\n",
    "\n",
    "study_results = {}\n",
    "\n",
    "def update_parallel_coordinate_plot(selected_model):\n",
    "    print(f\"Selected Model: {selected_model}\")\n",
    "    print(f\"Available Models: {models.keys()}\")\n",
    "\n",
    "for model_name, hyperparameters in models.items():\n",
    "    def objective(trial):\n",
    "        params = {}\n",
    "        for param_name, param_range in hyperparameters.items():\n",
    "            if param_name in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']:\n",
    "                params[param_name] = trial.suggest_int(param_name, *param_range)\n",
    "            else:\n",
    "                params[param_name] = trial.suggest_float(param_name, *param_range)\n",
    "\n",
    "        model = globals()[model_name](**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        return f1\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    study_results[model_name] = study\n",
    "\n",
    "@app.callback(\n",
    "    Output('parallel-coordinate-plot', 'figure'),\n",
    "    [Input('model-dropdown', 'value')]\n",
    ")\n",
    "\n",
    "def update_parallel_coordinate_plot(selected_model):\n",
    "    print(f\"Selected Model: {selected_model}\")\n",
    "    study = study_results[selected_model]\n",
    "    fig = ov.plot_parallel_coordinate(study)\n",
    "    fig.update_layout(title=f'Optimization Results for {selected_model}', autosize=False)\n",
    "    return fig\n",
    "\n",
    "app.run(port=9989, debug=True, jupyter_mode='external', allow_duplicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c435dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# dashboard 2 conde ends here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
